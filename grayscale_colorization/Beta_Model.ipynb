{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "from skimage.io import imsave\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "X = []\n",
    "for filename in os.listdir('./Train/'):\n",
    "    image = torchvision.io.read_image('./Train/'+filename)\n",
    "    # image = np.array(X, dtype=float)\n",
    "    X.append(image)\n",
    "    # print(image.shape)\n",
    "# print(X)\n",
    "X = np.array(X, dtype=float)\n",
    "# print(X)4\n",
    "\n",
    "# Test Dataset\n",
    "Y = []\n",
    "for filename in os.listdir('./Test/'):\n",
    "    image = torchvision.io.read_image('./Test/'+filename)\n",
    "    Y.append(image)\n",
    "Y = np.array(Y, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing test and train data\n",
    "split = int(0.95*len(X))\n",
    "Xtrain = X[:split]\n",
    "Xtrain = 1.0/255*Xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model defining\n",
    "class ColorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorModel, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3,padding=1),  # Assuming grayscale input images\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3,padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3,padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3,padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 2, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x * 128.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "epochs = 1\n",
    "folder_path_train = './Train/'\n",
    "folder_path_test = './Test/'\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = X\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = ColorModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the input array must have size 3 along `channel_axis`, got (8, 3, 256, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m      4\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 5\u001b[0m     lab_batch \u001b[38;5;241m=\u001b[39m \u001b[43mrgb2lab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     X_batch \u001b[38;5;241m=\u001b[39m lab_batch[:, :, :, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m     Y_batch \u001b[38;5;241m=\u001b[39m lab_batch[:, :, :, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m128\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\_shared\\utils.py:316\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\color\\colorconv.py:1253\u001b[0m, in \u001b[0;36mrgb2lab\u001b[1;34m(rgb, illuminant, observer, channel_axis)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;129m@channel_as_last_axis\u001b[39m()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrgb2lab\u001b[39m(rgb, illuminant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD65\u001b[39m\u001b[38;5;124m\"\u001b[39m, observer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m, channel_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Conversion from the sRGB color space (IEC 61966-2-1:1999)\u001b[39;00m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;124;03m    to the CIE Lab colorspace under the given illuminant and observer.\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;124;03m    .. [1] https://en.wikipedia.org/wiki/Standard_illuminant\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xyz2lab(\u001b[43mrgb2xyz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m, illuminant, observer)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\_shared\\utils.py:316\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\color\\colorconv.py:824\u001b[0m, in \u001b[0;36mrgb2xyz\u001b[1;34m(rgb, channel_axis)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"RGB to XYZ color space conversion.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m \n\u001b[0;32m    785\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;124;03m>>> img_xyz = rgb2xyz(img)\u001b[39;00m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# Follow the algorithm from http://www.easyrgb.com/index.php\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# except we don't multiply/divide by 100 in the conversion\u001b[39;00m\n\u001b[1;32m--> 824\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_colorarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    825\u001b[0m mask \u001b[38;5;241m=\u001b[39m arr \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.04045\u001b[39m\n\u001b[0;32m    826\u001b[0m arr[mask] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpower((arr[mask] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.055\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1.055\u001b[39m, \u001b[38;5;241m2.4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\color\\colorconv.py:154\u001b[0m, in \u001b[0;36m_prepare_colorarray\u001b[1;34m(arr, force_copy, channel_axis)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mshape[channel_axis] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    152\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe input array must have size 3 along `channel_axis`, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    153\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    156\u001b[0m float_dtype \u001b[38;5;241m=\u001b[39m _supported_float_type(arr\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m float_dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n",
      "\u001b[1;31mValueError\u001b[0m: the input array must have size 3 along `channel_axis`, got (8, 3, 256, 256)"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        lab_batch = rgb2lab(np.array(batch))\n",
    "        X_batch = lab_batch[:, :, :, 0]\n",
    "        Y_batch = lab_batch[:, :, :, 1:] / 128\n",
    "        X_batch = torch.Tensor(X_batch).unsqueeze(1)  # Add channel dimension\n",
    "        Y_batch = torch.Tensor(Y_batch)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, Y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ColorModel.__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mColorModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mTypeError\u001b[0m: ColorModel.__init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "test_dataset = ColorModel(folder_path_test, transform=data_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        lab_batch = rgb2lab(np.array(batch))\n",
    "        X_batch = lab_batch[:, :, :, 0]\n",
    "        X_batch = torch.Tensor(X_batch).unsqueeze(1)  # Add channel dimension\n",
    "        output = model(X_batch)\n",
    "        output = output * 128\n",
    "\n",
    "        # Output colorizations\n",
    "        for j in range(output.size(0)):\n",
    "            cur = np.zeros((256, 256, 3))\n",
    "            cur[:, :, 0] = X_batch[j, 0].numpy()\n",
    "            cur[:, :, 1:] = output[j].numpy()\n",
    "            imsave(f\"result/img_{i * batch_size + j}.png\", lab2rgb(cur))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
